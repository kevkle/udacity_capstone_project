{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise I will build a model to classify social media comments. The purpose is to train a NLP-classifier for sentiment analysis to detect whether those comments are hateful, racist or toxic insults. This will be done for 2 datasets and 2 different kind of models. \n",
    "\n",
    "We are working wit two datasets, which are very similar.\n",
    "\n",
    "1. Detecting Insults in Social Commentary\n",
    "2. Toxic Comment Classification Challenge\n",
    "\n",
    "Both datasets come from Kaggle and contain social media comments. The difference for this datasets are the different target variables.\n",
    "\n",
    "In the first dataset there is only one target variable, which detects whether a tweet or comment is an insult. This is a binary classification problem so the neural network will have one final neuron with a sigmoid activation function to predict the target variable.\n",
    " \n",
    "The second dataset contains 6 different target variables, which are called \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity hate\". So in this scenario a wider range of insults are adressed. It is important to mention that the occurences of those target variables are not mutually exclusive. However, the task of this assignment is to predict whether a comment hits a category. In this problem we will have a 6 unit sigmoid output layer, where each output unit determines whether the corresponding category is hit. \n",
    "\n",
    "The exercise will be split in the following sections, which will be repeated for either dataset  \n",
    "1. Load pacoages and data: This step is the same for both datasets, although the packages are just loaded ones.\n",
    "2. Data preprocessing: This section contains several functions, which clean the dataset and prepare it to be used in a meaningful way in the modelling step. \n",
    "3. Word embeddings and sentence indices: The word embeddings are loaded and the sentences are prepared so that they can be used as inputs for our RNN model.\n",
    "4. Model for dataset 1: A RNN  model with GRU blocks\n",
    "5. Model for dataset 2: Same model applied for the second dataset with some adjustments  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from numpy import genfromtxt\n",
    "\n",
    "\n",
    "#nltk.download('book')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.book import *\n",
    "#from gensim.models import Word2Vec\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The share of non insult comments is 73.42 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>\"@SDL OK, but I would hope they'd sign him to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Yeah and where are you now?\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>\"shut the fuck up. you and the rest of your fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Either you are fake or extremely stupid...may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>\"That you are an idiot who understands neither...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult                                            Comment\n",
       "0       1                               \"You fuck your dad.\"\n",
       "1       0  \"i really don't understand your point.\\xa0 It ...\n",
       "2       0  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
       "3       0  \"listen if you dont wanna get married to a man...\n",
       "4       0  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...\n",
       "5       0  \"@SDL OK, but I would hope they'd sign him to ...\n",
       "6       0                      \"Yeah and where are you now?\"\n",
       "7       1  \"shut the fuck up. you and the rest of your fa...\n",
       "8       1  \"Either you are fake or extremely stupid...may...\n",
       "9       1  \"That you are an idiot who understands neither..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train_data1.csv\")\n",
    "del train_data['Date']\n",
    "\n",
    "\n",
    "\n",
    "share = sum(train_data['Insult'] == 0) / len(train_data['Insult'])\n",
    "\n",
    "print(\"The share of non insult comments is\", round(share,4) * 100, \"%\")\n",
    "train_data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section several functions are defined to clean and preprocess the dataset for further use.\n",
    "\n",
    "In order to be consistent with writing certain phrases we will implement the function $decontracted()$, which will convert phrases like \"don't\" to \"do not\" etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    \n",
    "    \"\"\"\n",
    "    function that takes as input the most used english phrases and expands them to the actual\n",
    "    words\n",
    "    \n",
    "    Input: \n",
    "    phrase - Phrases like \"won't\" or \"don't\"\n",
    "\n",
    "    Returns: \n",
    "    The same phrase expanded to \"will not\" and \"do not\" respectively\n",
    "    \"\"\"\n",
    "    \n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"dont\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r'\\w*@\\w*','', phrase)\n",
    "    \n",
    "    return phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence is going to be: I do not like this movie, I will not watch it again\n"
     ]
    }
   ],
   "source": [
    "phrase = decontracted(\"I don't like this movie, I won't watch it again\")\n",
    "print('The sentence is going to be:',phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function is used to make the data useable. This means that words are going to be split at stoping signs and all words are going to be written in lower case etc. Also the function $decontracted$ will be used in this function called $preproc$. At this point we are just preprocessing the data and are not investigating whether the sentences make sense, this will be adressed in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preproc(data,word):\n",
    "\n",
    "\n",
    "    sen = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        sen.append(re.split(' |\\\\\\\\n|\\\\\\\\|\\n\\n|\\n|xc2|xa0|x80|xe2|!|\"|\\.(?!\\d)|\\?(?!\\d)|-|,',data[word][i]))\n",
    "\n",
    "    for i in range(len(data)):  \n",
    "        sen[i] = [word.lower() for word in sen[i]]\n",
    "        sen[i] = [decontracted(word) for word in sen[i]]\n",
    "    \n",
    "    punct = list(punctuation)\n",
    "    punct.append('``')\n",
    "    punct.append(\"''\")\n",
    "    punct.append('--')\n",
    "    punct.append('...')\n",
    "    punct.append('')\n",
    "    punct.append(',')\n",
    "    punct.append(\"'\")\n",
    "\n",
    "    sentences = []\n",
    "    \n",
    "    for i in range(len(sen)):\n",
    "        sentences.append([word for word in sen[i] if word not in punct])\n",
    "\n",
    "        \n",
    "    data = [' '.join(i) for i in sentences]\n",
    "    data = np.asarray(data)    \n",
    "    \n",
    "    \n",
    "    [data[i].split() for i in range(len(data))]\n",
    "        \n",
    "    return sen, sentences, data  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_length(data):\n",
    "\n",
    "    length = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        length.append(len(data[i].split()))\n",
    "    \n",
    "    max_len = max(length)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['you', 'fuck', 'your', 'dad'],\n",
       " ['i',\n",
       "  'really',\n",
       "  'do',\n",
       "  'not',\n",
       "  'understand',\n",
       "  'your',\n",
       "  'point',\n",
       "  'it',\n",
       "  'seems',\n",
       "  'that',\n",
       "  'you',\n",
       "  'are',\n",
       "  'mixing',\n",
       "  'apples',\n",
       "  'and',\n",
       "  'oranges'],\n",
       " ['a',\n",
       "  'majority',\n",
       "  'of',\n",
       "  'canadians',\n",
       "  'can',\n",
       "  'and',\n",
       "  'has',\n",
       "  'been',\n",
       "  'wrong',\n",
       "  'before',\n",
       "  'now',\n",
       "  'and',\n",
       "  'will',\n",
       "  'be',\n",
       "  'again',\n",
       "  'unless',\n",
       "  'you',\n",
       "  'are',\n",
       "  'supportive',\n",
       "  'of',\n",
       "  'the',\n",
       "  'idea',\n",
       "  'that',\n",
       "  'nothing',\n",
       "  'is',\n",
       "  'full',\n",
       "  'proof',\n",
       "  'or',\n",
       "  'perfect',\n",
       "  'so',\n",
       "  'you',\n",
       "  'take',\n",
       "  'your',\n",
       "  'chances',\n",
       "  'and',\n",
       "  'if',\n",
       "  'we',\n",
       "  'should',\n",
       "  'inadvertently',\n",
       "  'kill',\n",
       "  'your',\n",
       "  'son',\n",
       "  'or',\n",
       "  'daughter',\n",
       "  'then',\n",
       "  'them',\n",
       "  'is',\n",
       "  'the',\n",
       "  'breaks',\n",
       "  'and',\n",
       "  'we',\n",
       "  'can',\n",
       "  'always',\n",
       "  'regard',\n",
       "  'you',\n",
       "  'as',\n",
       "  'collateral',\n",
       "  'damage',\n",
       "  'like',\n",
       "  'in',\n",
       "  'wartime',\n",
       "  'and',\n",
       "  'sorry',\n",
       "  'but',\n",
       "  'the',\n",
       "  'cheques',\n",
       "  'in',\n",
       "  'the',\n",
       "  'mail'],\n",
       " ['listen',\n",
       "  'if',\n",
       "  'you',\n",
       "  'do',\n",
       "  'not',\n",
       "  'wanna',\n",
       "  'get',\n",
       "  'married',\n",
       "  'to',\n",
       "  'a',\n",
       "  'man',\n",
       "  'or',\n",
       "  'a',\n",
       "  'women',\n",
       "  'do',\n",
       "  'not',\n",
       "  'do',\n",
       "  'it',\n",
       "  'what',\n",
       "  'would',\n",
       "  'it',\n",
       "  'bother',\n",
       "  'you',\n",
       "  'if',\n",
       "  'gay',\n",
       "  'people',\n",
       "  'got',\n",
       "  'married',\n",
       "  'stay',\n",
       "  'in',\n",
       "  'your',\n",
       "  'lane',\n",
       "  'do',\n",
       "  'you',\n",
       "  'let',\n",
       "  'them',\n",
       "  'do',\n",
       "  'them',\n",
       "  'and',\n",
       "  'your',\n",
       "  'god',\n",
       "  'is',\n",
       "  'so',\n",
       "  'nice',\n",
       "  'but',\n",
       "  'quick',\n",
       "  'to',\n",
       "  'judg',\n",
       "  'if',\n",
       "  'your',\n",
       "  'not',\n",
       "  'like',\n",
       "  'him',\n",
       "  'thought',\n",
       "  'you',\n",
       "  'wasnt',\n",
       "  'suppose',\n",
       "  'to',\n",
       "  'judge',\n",
       "  'people'],\n",
       " ['c',\n",
       "  'xe1c',\n",
       "  'b',\n",
       "  'u1ea1n',\n",
       "  'xu',\n",
       "  'u1ed1ng',\n",
       "  'u0111',\n",
       "  'u01b0',\n",
       "  'u1eddng',\n",
       "  'bi',\n",
       "  'u1ec3u',\n",
       "  't',\n",
       "  'xecnh',\n",
       "  '2011',\n",
       "  'c',\n",
       "  'xf3',\n",
       "  'xf4n',\n",
       "  'ho',\n",
       "  'xe0',\n",
       "  'kh',\n",
       "  'xf4ng',\n",
       "  'c',\n",
       "  'xe1c',\n",
       "  'ng',\n",
       "  'u01b0',\n",
       "  'd',\n",
       "  'n',\n",
       "  'ng',\n",
       "  'u1ed3i',\n",
       "  'cu',\n",
       "  'xed',\n",
       "  'u0111',\n",
       "  'u1ea7u',\n",
       "  'chi',\n",
       "  'u1ee5',\n",
       "  'nh',\n",
       "  'u1ee5c',\n",
       "  'c',\n",
       "  'xf3',\n",
       "  'xf4n',\n",
       "  'ho',\n",
       "  'xe0',\n",
       "  'kh',\n",
       "  'xf4ng',\n",
       "  'c',\n",
       "  'xe1c',\n",
       "  'n',\n",
       "  'xf4ng',\n",
       "  'd',\n",
       "  'n',\n",
       "  'gi',\n",
       "  'u1eef',\n",
       "  'u0111',\n",
       "  'u1ea5t',\n",
       "  'u1edf',\n",
       "  'v',\n",
       "  'u0103n',\n",
       "  'giang',\n",
       "  'c',\n",
       "  'u1ea7n',\n",
       "  'th',\n",
       "  'u01a1',\n",
       "  'c',\n",
       "  'xf3',\n",
       "  'xf4n',\n",
       "  'ho',\n",
       "  'xe0',\n",
       "  'kh',\n",
       "  'xf4ng',\n",
       "  'r',\n",
       "  'u1ed1t',\n",
       "  'cu',\n",
       "  'u1ed9c',\n",
       "  'u0111',\n",
       "  'u01b0',\n",
       "  'u1ee3c',\n",
       "  'g',\n",
       "  'xec',\n",
       "  'th',\n",
       "  'xec',\n",
       "  'ch',\n",
       "  'xfang',\n",
       "  'ta',\n",
       "  'u0111',\n",
       "  'xe3',\n",
       "  'bi',\n",
       "  'u1ebft',\n",
       "  'ai',\n",
       "  'c',\n",
       "  'u0169ng',\n",
       "  'y',\n",
       "  'xeau',\n",
       "  'chu',\n",
       "  'u1ed9ng',\n",
       "  'ho',\n",
       "  'xe0',\n",
       "  'b',\n",
       "  'xecnh',\n",
       "  'nh',\n",
       "  'u01b0ng',\n",
       "  'u0111',\n",
       "  'xf4i',\n",
       "  'khi',\n",
       "  'ho',\n",
       "  'xe0',\n",
       "  'b',\n",
       "  'xecnh',\n",
       "  'ch',\n",
       "  'u1ec9',\n",
       "  'th',\n",
       "  'u1eadt',\n",
       "  's',\n",
       "  'u1ef1',\n",
       "  'u0111',\n",
       "  'u1ebfn',\n",
       "  'sau',\n",
       "  'chi',\n",
       "  'u1ebfn',\n",
       "  'tranh',\n",
       "  'm',\n",
       "  'xe0',\n",
       "  'th',\n",
       "  'xf4i',\n",
       "  'kh',\n",
       "  'xf4ng',\n",
       "  'c',\n",
       "  'xf2n',\n",
       "  'con',\n",
       "  'u0111',\n",
       "  'u01b0',\n",
       "  'u1eddng',\n",
       "  'n',\n",
       "  'xe0o',\n",
       "  'ch',\n",
       "  'u1ecdn',\n",
       "  'kh',\n",
       "  'xe1c',\n",
       "  'u0111',\n",
       "  'u',\n",
       "  'u0111',\n",
       "  'u1eebng',\n",
       "  'm',\n",
       "  'u01a1',\n",
       "  'th',\n",
       "  'xeam',\n",
       "  'n',\n",
       "  'u01b0',\n",
       "  'xe3']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen, sentences, data = preproc(train_data,'Comment')\n",
    "\n",
    "[data[i].split() for i in range(len(data))][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 16683 samples and 132655 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 4814),\n",
       " ('you', 3729),\n",
       " ('to', 3248),\n",
       " ('a', 2971),\n",
       " ('and', 2824),\n",
       " ('of', 2240),\n",
       " ('i', 1813),\n",
       " ('is', 1705),\n",
       " ('that', 1598),\n",
       " ('are', 1565),\n",
       " ('in', 1544),\n",
       " ('it', 1355),\n",
       " ('your', 1281),\n",
       " ('for', 1073),\n",
       " ('on', 931),\n",
       " ('have', 890),\n",
       " ('not', 862),\n",
       " ('be', 764),\n",
       " ('they', 747),\n",
       " ('this', 738)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sentences[0]\n",
    "\n",
    "for i in range(1,len(sentences)):\n",
    "    words +=  sentences[i] \n",
    "\n",
    "print(FreqDist(words))\n",
    "FreqDist(words).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word Embeddings & sentence indices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to define the dictionary of the word embedding to be used as input for the model later and the positions of the words in the dictionary as input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_word_vectors(file_path):\n",
    "    \n",
    "    with open(file_path,'r') as vec:\n",
    "        words = []\n",
    "        words_to_vec = {}\n",
    "        for line in vec:\n",
    "            line = line.split()\n",
    "            current_word = line[0]\n",
    "            words.append(current_word)\n",
    "            words_to_vec[current_word] = np.array(line[1:],dtype = 'float64')\n",
    "        \n",
    "        index_to_word = {}\n",
    "        word_to_index = {}\n",
    "    \n",
    "        i = 1\n",
    "    \n",
    "        for word in sorted(words):\n",
    "            index_to_word[i] = word\n",
    "            word_to_index[word] = i\n",
    "            i = i + 1\n",
    "\n",
    "    return words_to_vec, word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = 'glove.6B/glove.6B.100d.txt'\n",
    "\n",
    "words_to_vec, word_to_index, index_to_word = read_word_vectors(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_index(data,word_to_index,max_len,temp):\n",
    "    \n",
    "    \"\"\"\n",
    "    function that takes a sentences and gives the vector of indices back for all the words in the sentence\n",
    "    \n",
    "    Input: \n",
    "    data ... That is the data set, which contains the sentences to be translated to indices \n",
    "    word_to_index ... The dictionary that holds the index of any word in the word embedding \n",
    "    max_len... Maximum length of a sentence. If a sentence does not have maximum length, then the additional fields \n",
    "                are filled with zeros \n",
    "    temp ... This function is used twice\n",
    "                1. Identify how many sentences have words which are not in the Glove6B embedding. If there are too\n",
    "                   many unidentifable words, then this sentences will be taken out of the data (temp == 0)\n",
    "                2. For the actual indexing of the vectors to find out the word vector of certain words.\n",
    "                \n",
    "    Output:\n",
    "    Index_vector ... A matrix which returns the index of every word of a sentence in the corresponding word embedding\n",
    "    \"\"\"\n",
    "\n",
    "    m = data.shape[0] # number of traing examples\n",
    "    index_vector = np.zeros((m,max_len),dtype = 'int32') # Matrix of all sentence examples and corresponding indices\n",
    "    \n",
    "    for i in range(m):\n",
    "        # Standardize all words in the sentence to lower case and split them \n",
    "        sentence_words = data[i].lower().split()\n",
    "        \n",
    "        j = 0\n",
    "        \n",
    "        for word in sentence_words:\n",
    "            if word in word_to_index.keys():\n",
    "                index_vector[i,j] = word_to_index[word]\n",
    "            elif temp == 0:\n",
    "                index_vector[i,j] = -1\n",
    "            else:\n",
    "                index_vector[i,j] = 0\n",
    "            j = j + 1\n",
    "              \n",
    "    return index_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are to many words in a comment which are not part of the dictionary these comments will be taken out of the dataset. The reason is that the model is learning on reliable and meaningful data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordclean(vector,threshold):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    minus = []\n",
    "\n",
    "    for i in range(len(vector)):\n",
    "        minus.append(sum(vector[i] == -1))\n",
    "    \n",
    "    index = []\n",
    "\n",
    "    for i in range(len(minus)):\n",
    "        index.append(minus[i] < threshold)\n",
    "  \n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funny', 'and', 'very', 'honest', 'part', 'of', 'me', 'thinks', 'ms', 'bell', 'is', 'very', 'gutsy', 'for', 'sharing', 'that', 'video', 'but', 'the', 'other', 'part', 'tells', 'me', 'that', 'losing', 'one', 'is', 'sh*t', 'over', 'a', 'petting', 'zoo', 'critter', 'is', 'just', 'a', 'little', 'this', 'side', 'of', 'weird']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([155345,  54718, 377946, ...,      0,      0,      0], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len1 = word_length(data)\n",
    "\n",
    "vector = sentence_to_index(data,word_to_index,max_len = max_len1,temp = 0)\n",
    "\n",
    "\n",
    "print(data[2000].split())\n",
    "vector[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733\n"
     ]
    }
   ],
   "source": [
    "index = wordclean(vector, threshold = 10) \n",
    "data = data[index]\n",
    "\n",
    "max_len2 = word_length(data)\n",
    "print(max_len2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNNprep(original_data,processed_data,index,n,word):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    target = original_data[word][index]\n",
    "    target = np.asarray(target)\n",
    "  \n",
    "\n",
    "    a = np.zeros(len(processed_data))\n",
    "\n",
    "    for i in range(0,len(processed_data)):\n",
    "        a[i] = len(processed_data[i].split())\n",
    "\n",
    "    print(sum(a<=n) / len(processed_data))    \n",
    "    \n",
    "    processed_data = processed_data[a <= n]\n",
    "    target = target[a <=n]\n",
    "\n",
    "#####################################\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'Comment': processed_data, 'y':target})\n",
    "\n",
    "    X = df['Comment']\n",
    "    y = df['y']\n",
    "\n",
    "\n",
    "    max_len2 = word_length(processed_data)\n",
    "\n",
    "    \n",
    "    return X,y,max_len2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.907312049434\n",
      "The maximum length of the sentences is going to be: 70\n"
     ]
    }
   ],
   "source": [
    "X, y, max_len2 = RNNprep(train_data,data,index,70, 'Insult')\n",
    "print('The maximum length of the sentences is going to be:',max_len2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, LeakyReLU,GRU,Flatten,MaxPooling1D,Bidirectional,GlobalMaxPooling1D,Conv1D,Conv2D, MaxPooling2D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import class_weight\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are adding the embedding layer that is going to use the first layer after the input layer to transform the index vector to their corresponding word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trained_embedding_layer(words_to_vec,word_to_index):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    embedding_dim = words_to_vec['aha'].shape[0]\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len,embedding_dim))\n",
    "    \n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = words_to_vec[word]\n",
    "        \n",
    "        \n",
    "    embedding_layer = Embedding(input_dim = vocab_len, output_dim = embedding_dim, trainable = False)\n",
    "    \n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_shape = (max_len2,)\n",
    "\n",
    "def RNN(input_shape, words_to_vec, word_to_index):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = trained_embedding_layer(words_to_vec, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    X = Dropout(0.3)(embeddings)\n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    X = GRU(units = 64, activation = 'tanh')(X)\n",
    "    # Add dropout with a probability of 0.7\n",
    "    X = Dropout(0.6)(X)\n",
    "\n",
    "    # Problem with overfitting, so we are adding regularization and ease the architecture, playing around with \n",
    "    # different hyperparameter settings\n",
    "    \n",
    "    #X = LSTM(units = 128, activation = 'tanh')(X)\n",
    "    # Add dropout with a probability of 0.7\n",
    "    #X = Dropout(0.8)(X)\n",
    "    # Propagate X through a Dense layer with sigmoid activation to get back a batch of 1-dim vectors.\n",
    "    X = Dense(units = 1)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('sigmoid')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs = sentence_indices, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model specifications that are going to be used to optimize the model and find the best solution for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 70, 100)           40000100  \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 70, 100)           0         \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 64)                31680     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 40,031,845\n",
      "Trainable params: 31,745\n",
      "Non-trainable params: 40,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = RNN((max_len2,), words_to_vec, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.001,decay = 10e-3)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_index = sentence_to_index(np.asarray(X_train),word_to_index,max_len2,temp = 1)\n",
    "Y_train_index = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2853 samples, validate on 318 samples\n",
      "Epoch 1/200\n",
      "2853/2853 [==============================] - 2s 841us/step - loss: 0.6657 - acc: 0.7182 - val_loss: 0.6189 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.73270, saving model to model1_check.h5\n",
      "Epoch 2/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.6082 - acc: 0.7199 - val_loss: 0.5848 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.73270\n",
      "Epoch 3/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.5970 - acc: 0.7203 - val_loss: 0.5808 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.73270\n",
      "Epoch 4/200\n",
      "2853/2853 [==============================] - 1s 432us/step - loss: 0.5927 - acc: 0.7199 - val_loss: 0.5805 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.73270\n",
      "Epoch 5/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.5942 - acc: 0.7199 - val_loss: 0.5821 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.73270\n",
      "Epoch 6/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.5946 - acc: 0.7210 - val_loss: 0.5806 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.73270\n",
      "Epoch 7/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.5934 - acc: 0.7206 - val_loss: 0.5808 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.73270\n",
      "Epoch 8/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.5924 - acc: 0.7203 - val_loss: 0.5798 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.73270\n",
      "Epoch 9/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.5912 - acc: 0.7199 - val_loss: 0.5809 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.73270\n",
      "Epoch 10/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.5881 - acc: 0.7203 - val_loss: 0.5781 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.73270\n",
      "Epoch 11/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.5875 - acc: 0.7227 - val_loss: 0.5770 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.73270\n",
      "Epoch 12/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.5844 - acc: 0.7227 - val_loss: 0.5747 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.73270\n",
      "Epoch 13/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.5799 - acc: 0.7238 - val_loss: 0.5595 - val_acc: 0.7296\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.73270\n",
      "Epoch 14/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.5603 - acc: 0.7234 - val_loss: 0.5279 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.73270 to 0.73585, saving model to model1_check.h5\n",
      "Epoch 15/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.5508 - acc: 0.7238 - val_loss: 0.5203 - val_acc: 0.7264\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.73585\n",
      "Epoch 16/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.5384 - acc: 0.7231 - val_loss: 0.5051 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.73585\n",
      "Epoch 17/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.5206 - acc: 0.7249 - val_loss: 0.5016 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.73585\n",
      "Epoch 18/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.5161 - acc: 0.7220 - val_loss: 0.5073 - val_acc: 0.7390\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.73585 to 0.73899, saving model to model1_check.h5\n",
      "Epoch 19/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.5078 - acc: 0.7238 - val_loss: 0.4926 - val_acc: 0.7390\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.73899\n",
      "Epoch 20/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.4826 - acc: 0.7308 - val_loss: 0.4854 - val_acc: 0.7579\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.73899 to 0.75786, saving model to model1_check.h5\n",
      "Epoch 21/200\n",
      "2853/2853 [==============================] - 1s 433us/step - loss: 0.4814 - acc: 0.7399 - val_loss: 0.4702 - val_acc: 0.7642\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.75786 to 0.76415, saving model to model1_check.h5\n",
      "Epoch 22/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.4723 - acc: 0.7448 - val_loss: 0.4595 - val_acc: 0.7862\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.76415 to 0.78616, saving model to model1_check.h5\n",
      "Epoch 23/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.4603 - acc: 0.7624 - val_loss: 0.4565 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.78616 to 0.79874, saving model to model1_check.h5\n",
      "Epoch 24/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.4602 - acc: 0.7795 - val_loss: 0.4432 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.79874 to 0.80503, saving model to model1_check.h5\n",
      "Epoch 25/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.4503 - acc: 0.7886 - val_loss: 0.4402 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.80503\n",
      "Epoch 26/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.4470 - acc: 0.7981 - val_loss: 0.4342 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.80503 to 0.81447, saving model to model1_check.h5\n",
      "Epoch 27/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.4377 - acc: 0.7928 - val_loss: 0.4280 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.81447\n",
      "Epoch 28/200\n",
      "2853/2853 [==============================] - 1s 452us/step - loss: 0.4393 - acc: 0.7932 - val_loss: 0.4291 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.81447\n",
      "Epoch 29/200\n",
      "2853/2853 [==============================] - 1s 441us/step - loss: 0.4358 - acc: 0.8044 - val_loss: 0.4222 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.81447\n",
      "Epoch 30/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.4272 - acc: 0.8139 - val_loss: 0.4210 - val_acc: 0.7830\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.81447\n",
      "Epoch 31/200\n",
      "2853/2853 [==============================] - 1s 440us/step - loss: 0.4264 - acc: 0.8146 - val_loss: 0.4185 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.81447\n",
      "Epoch 32/200\n",
      "2853/2853 [==============================] - 1s 441us/step - loss: 0.4217 - acc: 0.8132 - val_loss: 0.4154 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.81447\n",
      "Epoch 33/200\n",
      "2853/2853 [==============================] - 1s 442us/step - loss: 0.4133 - acc: 0.8128 - val_loss: 0.4181 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.81447\n",
      "Epoch 34/200\n",
      "2853/2853 [==============================] - 1s 441us/step - loss: 0.4179 - acc: 0.8198 - val_loss: 0.4146 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.81447\n",
      "Epoch 35/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.4114 - acc: 0.8198 - val_loss: 0.4143 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.81447 to 0.81761, saving model to model1_check.h5\n",
      "Epoch 36/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.4064 - acc: 0.8174 - val_loss: 0.4084 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.81761\n",
      "Epoch 37/200\n",
      "2853/2853 [==============================] - 1s 433us/step - loss: 0.3983 - acc: 0.8332 - val_loss: 0.4182 - val_acc: 0.7893\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.81761\n",
      "Epoch 38/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.3998 - acc: 0.8268 - val_loss: 0.4121 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.81761\n",
      "Epoch 39/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3933 - acc: 0.8297 - val_loss: 0.4092 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.81761\n",
      "Epoch 40/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3965 - acc: 0.8325 - val_loss: 0.4084 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.81761\n",
      "Epoch 41/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3936 - acc: 0.8370 - val_loss: 0.4080 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.81761\n",
      "Epoch 42/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3871 - acc: 0.8339 - val_loss: 0.4049 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.81761\n",
      "Epoch 43/200\n",
      "2853/2853 [==============================] - 1s 433us/step - loss: 0.3905 - acc: 0.8321 - val_loss: 0.3989 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.81761 to 0.82704, saving model to model1_check.h5\n",
      "Epoch 44/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.3900 - acc: 0.8293 - val_loss: 0.4048 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.82704\n",
      "Epoch 45/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3784 - acc: 0.8335 - val_loss: 0.3959 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.82704\n",
      "Epoch 46/200\n",
      "2853/2853 [==============================] - 1s 455us/step - loss: 0.3886 - acc: 0.8332 - val_loss: 0.3970 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.82704\n",
      "Epoch 47/200\n",
      "2853/2853 [==============================] - 1s 489us/step - loss: 0.3848 - acc: 0.8325 - val_loss: 0.3980 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.82704\n",
      "Epoch 48/200\n",
      "2853/2853 [==============================] - 1s 495us/step - loss: 0.3804 - acc: 0.8332 - val_loss: 0.3979 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.82704\n",
      "Epoch 49/200\n",
      "2853/2853 [==============================] - 1s 489us/step - loss: 0.3833 - acc: 0.8342 - val_loss: 0.4014 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.82704\n",
      "Epoch 50/200\n",
      "2853/2853 [==============================] - 1s 461us/step - loss: 0.3726 - acc: 0.8388 - val_loss: 0.3962 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.82704\n",
      "Epoch 51/200\n",
      "2853/2853 [==============================] - 1s 463us/step - loss: 0.3716 - acc: 0.8311 - val_loss: 0.4003 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.82704\n",
      "Epoch 52/200\n",
      "2853/2853 [==============================] - 1s 462us/step - loss: 0.3690 - acc: 0.8426 - val_loss: 0.4058 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.82704\n",
      "Epoch 53/200\n",
      "2853/2853 [==============================] - 1s 453us/step - loss: 0.3825 - acc: 0.8314 - val_loss: 0.4076 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.82704\n",
      "Epoch 54/200\n",
      "2853/2853 [==============================] - 1s 457us/step - loss: 0.3706 - acc: 0.8381 - val_loss: 0.3927 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.82704 to 0.83019, saving model to model1_check.h5\n",
      "Epoch 55/200\n",
      "2853/2853 [==============================] - 1s 456us/step - loss: 0.3744 - acc: 0.8384 - val_loss: 0.3937 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.83019\n",
      "Epoch 56/200\n",
      "2853/2853 [==============================] - 1s 448us/step - loss: 0.3757 - acc: 0.8342 - val_loss: 0.3899 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.83019\n",
      "Epoch 57/200\n",
      "2853/2853 [==============================] - 1s 450us/step - loss: 0.3719 - acc: 0.8405 - val_loss: 0.3956 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.83019\n",
      "Epoch 58/200\n",
      "2853/2853 [==============================] - 1s 445us/step - loss: 0.3635 - acc: 0.8482 - val_loss: 0.3940 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.83019\n",
      "Epoch 59/200\n",
      "2853/2853 [==============================] - 1s 453us/step - loss: 0.3656 - acc: 0.8475 - val_loss: 0.3982 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.83019\n",
      "Epoch 60/200\n",
      "2853/2853 [==============================] - 1s 448us/step - loss: 0.3700 - acc: 0.8370 - val_loss: 0.3878 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.83019\n",
      "Epoch 61/200\n",
      "2853/2853 [==============================] - 1s 448us/step - loss: 0.3609 - acc: 0.8440 - val_loss: 0.3858 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.83019\n",
      "Epoch 62/200\n",
      "2853/2853 [==============================] - 1s 451us/step - loss: 0.3550 - acc: 0.8517 - val_loss: 0.4008 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.83019\n",
      "Epoch 63/200\n",
      "2853/2853 [==============================] - 1s 451us/step - loss: 0.3582 - acc: 0.8468 - val_loss: 0.3866 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.83019\n",
      "Epoch 64/200\n",
      "2853/2853 [==============================] - 1s 451us/step - loss: 0.3655 - acc: 0.8440 - val_loss: 0.3857 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.83019\n",
      "Epoch 65/200\n",
      "2853/2853 [==============================] - 1s 447us/step - loss: 0.3645 - acc: 0.8426 - val_loss: 0.3828 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00065: val_acc improved from 0.83019 to 0.83333, saving model to model1_check.h5\n",
      "Epoch 66/200\n",
      "2853/2853 [==============================] - 1s 453us/step - loss: 0.3555 - acc: 0.8468 - val_loss: 0.3910 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.83333\n",
      "Epoch 67/200\n",
      "2853/2853 [==============================] - 1s 451us/step - loss: 0.3598 - acc: 0.8444 - val_loss: 0.3924 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.83333\n",
      "Epoch 68/200\n",
      "2853/2853 [==============================] - 1s 450us/step - loss: 0.3507 - acc: 0.8528 - val_loss: 0.3902 - val_acc: 0.8019\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.83333\n",
      "Epoch 69/200\n",
      "2853/2853 [==============================] - 1s 449us/step - loss: 0.3589 - acc: 0.8507 - val_loss: 0.3857 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.83333\n",
      "Epoch 70/200\n",
      "2853/2853 [==============================] - 1s 446us/step - loss: 0.3599 - acc: 0.8479 - val_loss: 0.3944 - val_acc: 0.7893\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.83333\n",
      "Epoch 71/200\n",
      "2853/2853 [==============================] - 1s 447us/step - loss: 0.3473 - acc: 0.8524 - val_loss: 0.3879 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.83333\n",
      "Epoch 72/200\n",
      "2853/2853 [==============================] - 1s 444us/step - loss: 0.3504 - acc: 0.8447 - val_loss: 0.3856 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.83333\n",
      "Epoch 73/200\n",
      "2853/2853 [==============================] - 1s 446us/step - loss: 0.3510 - acc: 0.8510 - val_loss: 0.3841 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.83333\n",
      "Epoch 74/200\n",
      "2853/2853 [==============================] - 1s 454us/step - loss: 0.3452 - acc: 0.8521 - val_loss: 0.3887 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.83333\n",
      "Epoch 75/200\n",
      "2853/2853 [==============================] - 1s 450us/step - loss: 0.3562 - acc: 0.8528 - val_loss: 0.3813 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.83333\n",
      "Epoch 76/200\n",
      "2853/2853 [==============================] - 1s 440us/step - loss: 0.3543 - acc: 0.8514 - val_loss: 0.3887 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.83333\n",
      "Epoch 77/200\n",
      "2853/2853 [==============================] - 1s 445us/step - loss: 0.3583 - acc: 0.8482 - val_loss: 0.3805 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.83333\n",
      "Epoch 78/200\n",
      "2853/2853 [==============================] - 1s 451us/step - loss: 0.3515 - acc: 0.8486 - val_loss: 0.3855 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.83333\n",
      "Epoch 79/200\n",
      "2853/2853 [==============================] - 1s 444us/step - loss: 0.3530 - acc: 0.8472 - val_loss: 0.3845 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.83333\n",
      "Epoch 80/200\n",
      "2853/2853 [==============================] - 1s 444us/step - loss: 0.3372 - acc: 0.8587 - val_loss: 0.3867 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.83333\n",
      "Epoch 81/200\n",
      "2853/2853 [==============================] - 1s 448us/step - loss: 0.3433 - acc: 0.8538 - val_loss: 0.3816 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.83333\n",
      "Epoch 82/200\n",
      "2853/2853 [==============================] - 1s 456us/step - loss: 0.3468 - acc: 0.8489 - val_loss: 0.3838 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.83333\n",
      "Epoch 83/200\n",
      "2853/2853 [==============================] - 1s 444us/step - loss: 0.3496 - acc: 0.8524 - val_loss: 0.3823 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.83333\n",
      "Epoch 84/200\n",
      "2853/2853 [==============================] - 1s 441us/step - loss: 0.3509 - acc: 0.8517 - val_loss: 0.3764 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.83333\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2853/2853 [==============================] - 1s 452us/step - loss: 0.3378 - acc: 0.8580 - val_loss: 0.3778 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.83333\n",
      "Epoch 86/200\n",
      "2853/2853 [==============================] - 1s 450us/step - loss: 0.3451 - acc: 0.8517 - val_loss: 0.3854 - val_acc: 0.8019\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.83333\n",
      "Epoch 87/200\n",
      "2853/2853 [==============================] - 1s 446us/step - loss: 0.3384 - acc: 0.8524 - val_loss: 0.3799 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.83333\n",
      "Epoch 88/200\n",
      "2853/2853 [==============================] - 1s 456us/step - loss: 0.3437 - acc: 0.8598 - val_loss: 0.3819 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.83333\n",
      "Epoch 89/200\n",
      "2853/2853 [==============================] - 1s 446us/step - loss: 0.3446 - acc: 0.8535 - val_loss: 0.3821 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.83333\n",
      "Epoch 90/200\n",
      "2853/2853 [==============================] - 1s 451us/step - loss: 0.3414 - acc: 0.8524 - val_loss: 0.3776 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.83333\n",
      "Epoch 91/200\n",
      "2853/2853 [==============================] - 1s 446us/step - loss: 0.3382 - acc: 0.8594 - val_loss: 0.3852 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.83333\n",
      "Epoch 92/200\n",
      "2853/2853 [==============================] - 1s 451us/step - loss: 0.3405 - acc: 0.8524 - val_loss: 0.3724 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00092: val_acc improved from 0.83333 to 0.83648, saving model to model1_check.h5\n",
      "Epoch 93/200\n",
      "2853/2853 [==============================] - 1s 452us/step - loss: 0.3377 - acc: 0.8542 - val_loss: 0.3844 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.83648\n",
      "Epoch 94/200\n",
      "2853/2853 [==============================] - 1s 486us/step - loss: 0.3324 - acc: 0.8566 - val_loss: 0.3828 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.83648\n",
      "Epoch 95/200\n",
      "2853/2853 [==============================] - 1s 480us/step - loss: 0.3366 - acc: 0.8556 - val_loss: 0.3760 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.83648\n",
      "Epoch 96/200\n",
      "2853/2853 [==============================] - 1s 448us/step - loss: 0.3383 - acc: 0.8584 - val_loss: 0.3735 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.83648\n",
      "Epoch 97/200\n",
      "2853/2853 [==============================] - 1s 455us/step - loss: 0.3326 - acc: 0.8619 - val_loss: 0.3764 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.83648\n",
      "Epoch 98/200\n",
      "2853/2853 [==============================] - 1s 446us/step - loss: 0.3358 - acc: 0.8637 - val_loss: 0.3829 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.83648\n",
      "Epoch 99/200\n",
      "2853/2853 [==============================] - 1s 446us/step - loss: 0.3294 - acc: 0.8584 - val_loss: 0.3861 - val_acc: 0.7956\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.83648\n",
      "Epoch 100/200\n",
      "2853/2853 [==============================] - 1s 449us/step - loss: 0.3365 - acc: 0.8503 - val_loss: 0.3775 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.83648\n",
      "Epoch 101/200\n",
      "2853/2853 [==============================] - 1s 446us/step - loss: 0.3303 - acc: 0.8623 - val_loss: 0.3704 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.83648\n",
      "Epoch 102/200\n",
      "2853/2853 [==============================] - 1s 442us/step - loss: 0.3348 - acc: 0.8570 - val_loss: 0.3828 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.83648\n",
      "Epoch 103/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3278 - acc: 0.8605 - val_loss: 0.3761 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.83648\n",
      "Epoch 104/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3208 - acc: 0.8679 - val_loss: 0.3747 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.83648\n",
      "Epoch 105/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3272 - acc: 0.8615 - val_loss: 0.3753 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.83648\n",
      "Epoch 106/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.3395 - acc: 0.8580 - val_loss: 0.3751 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.83648\n",
      "Epoch 107/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3387 - acc: 0.8601 - val_loss: 0.3777 - val_acc: 0.8019\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.83648\n",
      "Epoch 108/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3281 - acc: 0.8675 - val_loss: 0.3752 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.83648\n",
      "Epoch 109/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3272 - acc: 0.8612 - val_loss: 0.3747 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.83648\n",
      "Epoch 110/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3317 - acc: 0.8573 - val_loss: 0.3724 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.83648\n",
      "Epoch 111/200\n",
      "2853/2853 [==============================] - 1s 441us/step - loss: 0.3386 - acc: 0.8531 - val_loss: 0.3711 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.83648\n",
      "Epoch 112/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3277 - acc: 0.8580 - val_loss: 0.3737 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.83648\n",
      "Epoch 113/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.3234 - acc: 0.8675 - val_loss: 0.3723 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.83648\n",
      "Epoch 114/200\n",
      "2853/2853 [==============================] - 1s 440us/step - loss: 0.3276 - acc: 0.8598 - val_loss: 0.3726 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.83648\n",
      "Epoch 115/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3252 - acc: 0.8637 - val_loss: 0.3727 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.83648\n",
      "Epoch 116/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3369 - acc: 0.8507 - val_loss: 0.3702 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.83648\n",
      "Epoch 117/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3355 - acc: 0.8531 - val_loss: 0.3723 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.83648\n",
      "Epoch 118/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3238 - acc: 0.8647 - val_loss: 0.3688 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.83648\n",
      "Epoch 119/200\n",
      "2853/2853 [==============================] - 1s 433us/step - loss: 0.3314 - acc: 0.8580 - val_loss: 0.3751 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.83648\n",
      "Epoch 120/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.3291 - acc: 0.8619 - val_loss: 0.3677 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.83648\n",
      "Epoch 121/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3299 - acc: 0.8658 - val_loss: 0.3772 - val_acc: 0.8019\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.83648\n",
      "Epoch 122/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3234 - acc: 0.8633 - val_loss: 0.3723 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.83648\n",
      "Epoch 123/200\n",
      "2853/2853 [==============================] - 1s 440us/step - loss: 0.3221 - acc: 0.8601 - val_loss: 0.3704 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.83648\n",
      "Epoch 124/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.3241 - acc: 0.8647 - val_loss: 0.3700 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.83648\n",
      "Epoch 125/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3193 - acc: 0.8601 - val_loss: 0.3717 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.83648\n",
      "Epoch 126/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3214 - acc: 0.8598 - val_loss: 0.3794 - val_acc: 0.8019\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.83648\n",
      "Epoch 127/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.3212 - acc: 0.8672 - val_loss: 0.3675 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.83648\n",
      "Epoch 128/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2853/2853 [==============================] - 1s 433us/step - loss: 0.3245 - acc: 0.8615 - val_loss: 0.3668 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.83648\n",
      "Epoch 129/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3264 - acc: 0.8549 - val_loss: 0.3698 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.83648\n",
      "Epoch 130/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.3205 - acc: 0.8689 - val_loss: 0.3666 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.83648\n",
      "Epoch 131/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3192 - acc: 0.8668 - val_loss: 0.3684 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.83648\n",
      "Epoch 132/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3226 - acc: 0.8601 - val_loss: 0.3719 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.83648\n",
      "Epoch 133/200\n",
      "2853/2853 [==============================] - 1s 441us/step - loss: 0.3167 - acc: 0.8626 - val_loss: 0.3699 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.83648\n",
      "Epoch 134/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3186 - acc: 0.8689 - val_loss: 0.3718 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.83648\n",
      "Epoch 135/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3201 - acc: 0.8689 - val_loss: 0.3657 - val_acc: 0.8365\n",
      "\n",
      "Epoch 00135: val_acc improved from 0.83648 to 0.83648, saving model to model1_check.h5\n",
      "Epoch 136/200\n",
      "2853/2853 [==============================] - 1s 440us/step - loss: 0.3221 - acc: 0.8675 - val_loss: 0.3653 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.83648\n",
      "Epoch 137/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.3122 - acc: 0.8654 - val_loss: 0.3732 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.83648\n",
      "Epoch 138/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3145 - acc: 0.8672 - val_loss: 0.3706 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.83648\n",
      "Epoch 139/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3226 - acc: 0.8608 - val_loss: 0.3647 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.83648\n",
      "Epoch 140/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3250 - acc: 0.8658 - val_loss: 0.3683 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.83648\n",
      "Epoch 141/200\n",
      "2853/2853 [==============================] - 1s 441us/step - loss: 0.3183 - acc: 0.8654 - val_loss: 0.3670 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.83648\n",
      "Epoch 142/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.3160 - acc: 0.8679 - val_loss: 0.3646 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.83648\n",
      "Epoch 143/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3105 - acc: 0.8714 - val_loss: 0.3724 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.83648\n",
      "Epoch 144/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3247 - acc: 0.8651 - val_loss: 0.3659 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.83648\n",
      "Epoch 145/200\n",
      "2853/2853 [==============================] - 1s 431us/step - loss: 0.3149 - acc: 0.8672 - val_loss: 0.3784 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.83648\n",
      "Epoch 146/200\n",
      "2853/2853 [==============================] - 1s 440us/step - loss: 0.3175 - acc: 0.8693 - val_loss: 0.3665 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.83648\n",
      "Epoch 147/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3213 - acc: 0.8630 - val_loss: 0.3709 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.83648\n",
      "Epoch 148/200\n",
      "2853/2853 [==============================] - 1s 433us/step - loss: 0.3105 - acc: 0.8717 - val_loss: 0.3658 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.83648\n",
      "Epoch 149/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3195 - acc: 0.8630 - val_loss: 0.3690 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.83648\n",
      "Epoch 150/200\n",
      "2853/2853 [==============================] - 1s 430us/step - loss: 0.3214 - acc: 0.8623 - val_loss: 0.3698 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.83648\n",
      "Epoch 151/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3189 - acc: 0.8658 - val_loss: 0.3669 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.83648\n",
      "Epoch 152/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3095 - acc: 0.8672 - val_loss: 0.3704 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.83648\n",
      "Epoch 153/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3133 - acc: 0.8689 - val_loss: 0.3699 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.83648\n",
      "Epoch 154/200\n",
      "2853/2853 [==============================] - 1s 431us/step - loss: 0.3181 - acc: 0.8619 - val_loss: 0.3713 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.83648\n",
      "Epoch 155/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3244 - acc: 0.8682 - val_loss: 0.3698 - val_acc: 0.8082\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.83648\n",
      "Epoch 156/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3198 - acc: 0.8651 - val_loss: 0.3613 - val_acc: 0.8396\n",
      "\n",
      "Epoch 00156: val_acc improved from 0.83648 to 0.83962, saving model to model1_check.h5\n",
      "Epoch 157/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.3148 - acc: 0.8721 - val_loss: 0.3722 - val_acc: 0.8113\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.83962\n",
      "Epoch 158/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3186 - acc: 0.8742 - val_loss: 0.3628 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.83962\n",
      "Epoch 159/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3118 - acc: 0.8731 - val_loss: 0.3702 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.83962\n",
      "Epoch 160/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3135 - acc: 0.8633 - val_loss: 0.3687 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.83962\n",
      "Epoch 161/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3083 - acc: 0.8749 - val_loss: 0.3683 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.83962\n",
      "Epoch 162/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3090 - acc: 0.8696 - val_loss: 0.3726 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.83962\n",
      "Epoch 163/200\n",
      "2853/2853 [==============================] - 1s 440us/step - loss: 0.3099 - acc: 0.8742 - val_loss: 0.3640 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.83962\n",
      "Epoch 164/200\n",
      "2853/2853 [==============================] - 1s 443us/step - loss: 0.3180 - acc: 0.8637 - val_loss: 0.3668 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.83962\n",
      "Epoch 165/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.3070 - acc: 0.8724 - val_loss: 0.3678 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.83962\n",
      "Epoch 166/200\n",
      "2853/2853 [==============================] - 1s 433us/step - loss: 0.3110 - acc: 0.8647 - val_loss: 0.3649 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.83962\n",
      "Epoch 167/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3033 - acc: 0.8742 - val_loss: 0.3686 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.83962\n",
      "Epoch 168/200\n",
      "2853/2853 [==============================] - 1s 433us/step - loss: 0.3078 - acc: 0.8742 - val_loss: 0.3652 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.83962\n",
      "Epoch 169/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3154 - acc: 0.8696 - val_loss: 0.3675 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.83962\n",
      "Epoch 170/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.3114 - acc: 0.8724 - val_loss: 0.3655 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.83962\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3176 - acc: 0.8651 - val_loss: 0.3640 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.83962\n",
      "Epoch 172/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3024 - acc: 0.8798 - val_loss: 0.3648 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.83962\n",
      "Epoch 173/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3085 - acc: 0.8703 - val_loss: 0.3748 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.83962\n",
      "Epoch 174/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.2999 - acc: 0.8805 - val_loss: 0.3620 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.83962\n",
      "Epoch 175/200\n",
      "2853/2853 [==============================] - 1s 433us/step - loss: 0.3083 - acc: 0.8749 - val_loss: 0.3693 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.83962\n",
      "Epoch 176/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3062 - acc: 0.8696 - val_loss: 0.3702 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.83962\n",
      "Epoch 177/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3069 - acc: 0.8717 - val_loss: 0.3646 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.83962\n",
      "Epoch 178/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3033 - acc: 0.8770 - val_loss: 0.3648 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.83962\n",
      "Epoch 179/200\n",
      "2853/2853 [==============================] - 1s 433us/step - loss: 0.3065 - acc: 0.8665 - val_loss: 0.3679 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.83962\n",
      "Epoch 180/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.3020 - acc: 0.8763 - val_loss: 0.3681 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.83962\n",
      "Epoch 181/200\n",
      "2853/2853 [==============================] - 1s 440us/step - loss: 0.3072 - acc: 0.8668 - val_loss: 0.3621 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.83962\n",
      "Epoch 182/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3044 - acc: 0.8728 - val_loss: 0.3630 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.83962\n",
      "Epoch 183/200\n",
      "2853/2853 [==============================] - 1s 444us/step - loss: 0.3033 - acc: 0.8707 - val_loss: 0.3714 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.83962\n",
      "Epoch 184/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.3214 - acc: 0.8714 - val_loss: 0.3650 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.83962\n",
      "Epoch 185/200\n",
      "2853/2853 [==============================] - 1s 439us/step - loss: 0.3147 - acc: 0.8598 - val_loss: 0.3645 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.83962\n",
      "Epoch 186/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3043 - acc: 0.8759 - val_loss: 0.3641 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.83962\n",
      "Epoch 187/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3078 - acc: 0.8728 - val_loss: 0.3617 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.83962\n",
      "Epoch 188/200\n",
      "2853/2853 [==============================] - 1s 438us/step - loss: 0.3030 - acc: 0.8714 - val_loss: 0.3658 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.83962\n",
      "Epoch 189/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3005 - acc: 0.8822 - val_loss: 0.3707 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.83962\n",
      "Epoch 190/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3084 - acc: 0.8721 - val_loss: 0.3646 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.83962\n",
      "Epoch 191/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3047 - acc: 0.8714 - val_loss: 0.3650 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.83962\n",
      "Epoch 192/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3145 - acc: 0.8721 - val_loss: 0.3678 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.83962\n",
      "Epoch 193/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3082 - acc: 0.8637 - val_loss: 0.3633 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.83962\n",
      "Epoch 194/200\n",
      "2853/2853 [==============================] - 1s 441us/step - loss: 0.3024 - acc: 0.8812 - val_loss: 0.3695 - val_acc: 0.8176\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.83962\n",
      "Epoch 195/200\n",
      "2853/2853 [==============================] - 1s 441us/step - loss: 0.2899 - acc: 0.8836 - val_loss: 0.3645 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.83962\n",
      "Epoch 196/200\n",
      "2853/2853 [==============================] - 1s 436us/step - loss: 0.3117 - acc: 0.8745 - val_loss: 0.3669 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.83962\n",
      "Epoch 197/200\n",
      "2853/2853 [==============================] - 1s 434us/step - loss: 0.3019 - acc: 0.8693 - val_loss: 0.3623 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.83962\n",
      "Epoch 198/200\n",
      "2853/2853 [==============================] - 1s 437us/step - loss: 0.3085 - acc: 0.8721 - val_loss: 0.3583 - val_acc: 0.8302\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.83962\n",
      "Epoch 199/200\n",
      "2853/2853 [==============================] - 1s 435us/step - loss: 0.2960 - acc: 0.8724 - val_loss: 0.3731 - val_acc: 0.8208\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.83962\n",
      "Epoch 200/200\n",
      "2853/2853 [==============================] - 1s 441us/step - loss: 0.3015 - acc: 0.8686 - val_loss: 0.3649 - val_acc: 0.8239\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.83962\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model1_check.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "temp1 = model.fit(X_train_index, Y_train_index, validation_split = 0.1,epochs = 200, batch_size = 128, callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('model1_check.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 0s 289us/step\n",
      "\n",
      "Test accuracy =  85.552407932 %\n"
     ]
    }
   ],
   "source": [
    "X_test_index = sentence_to_index(np.asarray(X_test), word_to_index, max_len = max_len2, temp = 1)\n",
    "#Y_test_index = np.eye(2)[np.asarray(y_test).reshape(-1)]\n",
    "Y_test_index = np.asarray(y_test)\n",
    "loss, acc = model.evaluate(X_test_index, Y_test_index)\n",
    "print()\n",
    "print(\"Test accuracy = \",acc * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compare it with the benchmark model, where we use p = 1 - share as parameter for our bernouilli variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_random = np.random.binomial(size=len(y_test), n=1, p = 1-share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random accuracy is 62.8895184136 %\n"
     ]
    }
   ],
   "source": [
    "# calculate the accuracy of the random benchmark model\n",
    "error = np.zeros(len(y_test))\n",
    "\n",
    "for i in range(0,len(y_test)):\n",
    "    error[i] = abs(y_random[i] - np.asarray(y_test)[i])\n",
    "    \n",
    "acc_random = 1 - sum(error)/len(y_test)\n",
    "\n",
    "print(\"The random accuracy is\",acc_random*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to predict and calculate the F1 score and predict comments we have not seen before. There we are going to use the sklearn function metrics.F1_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(phrases):\n",
    "    \n",
    "    m = phrases.shape[0]\n",
    "    \n",
    "    y_pred = model.predict(sentence_to_index(phrases, word_to_index, max_len2, temp = 1))\n",
    "    \n",
    "    predict = np.zeros(m)\n",
    "    \n",
    "    for i in range(0,m):\n",
    "        \n",
    "        if y_pred[i] > 0.5:\n",
    "            predict[i] = 1\n",
    "        else:\n",
    "            predict[i] = 0\n",
    "        predict\n",
    "        \n",
    "    return predict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.asarray(X_test)\n",
    "y_predict = predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1_score of the model is: 0.732984293194\n",
      "The F1_score of the benchmark is: 0.306878306878\n"
     ]
    }
   ],
   "source": [
    "F1_model  = sklearn.metrics.f1_score(np.asarray(y_test),y_predict)\n",
    "F1_random = sklearn.metrics.f1_score(np.asarray(y_test),y_random)\n",
    "\n",
    "print(\"The F1_score of the model is:\",F1_model)\n",
    "print(\"The F1_score of the benchmark is:\",F1_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  1.])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.asarray([\"I like you\",\"Go to hell\",\"You are a piece of shit\", \"Your mother is stupid\"])\n",
    "predict(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Data preprocessing for dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same procedure as for the first model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The share of category on dataset 0 is 9.5844 %\n",
      "The share of category on dataset 1 is 0.9996 %\n",
      "The share of category on dataset 2 is 5.2948 %\n",
      "The share of category on dataset 3 is 0.2996 %\n",
      "The share of category on dataset 4 is 4.9364 %\n",
      "The share of category on dataset 5 is 0.8805 %\n"
     ]
    }
   ],
   "source": [
    "train_data2 = pd.read_csv(\"train.csv\")\n",
    "del train_data2['id']\n",
    "\n",
    "rowSums = train_data2.sum(axis = 1)\n",
    "binary = []\n",
    "\n",
    "for i in range(0,len(train_data2)):\n",
    "    if rowSums[i] > 0:\n",
    "        binary.append(1)\n",
    "    else:\n",
    "        binary.append(0)\n",
    "\n",
    "category_sum = np.zeros(6)\n",
    "share2 = np.zeros(6)\n",
    "for i in range(0,6):\n",
    "    category_sum[i] = sum(train_data2.iloc[:,i+1])\n",
    "    share2[i] = category_sum[i] / len(train_data2)\n",
    "    print(\"The share of category on dataset\",i,\"is\",round(share2[i]*100,4),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The share of non sensitive comments is 89.83 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>sum</th>\n",
       "      <th>binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "5  \"\\n\\nCongratulations from me as well, use the ...      0             0   \n",
       "6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1             1   \n",
       "7  Your vandalism to the Matt Shirvington article...      0             0   \n",
       "8  Sorry if the word 'nonsense' was offensive to ...      0             0   \n",
       "9  alignment on this subject and which are contra...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  sum  binary  \n",
       "0        0       0       0              0    0       0  \n",
       "1        0       0       0              0    0       0  \n",
       "2        0       0       0              0    0       0  \n",
       "3        0       0       0              0    0       0  \n",
       "4        0       0       0              0    0       0  \n",
       "5        0       0       0              0    0       0  \n",
       "6        1       0       1              0    4       1  \n",
       "7        0       0       0              0    0       0  \n",
       "8        0       0       0              0    0       0  \n",
       "9        0       0       0              0    0       0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data2['sum'] = rowSums\n",
    "train_data2['binary'] = binary\n",
    "\n",
    "\n",
    "share2 = sum(train_data2['binary'] == 0) / len(train_data2['binary'])\n",
    "\n",
    "print(\"The share of non sensitive comments is\", round(share2,4) * 100,\"%\")\n",
    "train_data2[0:10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12f034390>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ef94198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data2['binary'].value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127737b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "objects = ('toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate')\n",
    "y_pos = np.arange(6)\n",
    "performance = category_sum\n",
    " \n",
    "    \n",
    "plt.figure(figsize=(8,5))          \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Number of occurences')\n",
    "plt.title('Occurence of violent categories')\n",
    " \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same functions as for the first model for the data preprocessing and preparation to be used for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen2, sentences2, data2 = preproc(train_data2,'comment_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len3 = word_length(data2)\n",
    "vector2 = sentence_to_index(data2,word_to_index,max_len = max_len3,temp = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1403\n"
     ]
    }
   ],
   "source": [
    "index2 = wordclean(vector2, threshold = 10) \n",
    "data2 = data2[index2]\n",
    "\n",
    "max_len4 = word_length(data2)\n",
    "print(max_len4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNNprep2(original_data,processed_data,index,n,word):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    target = original_data[index]\n",
    "   # target = np.asarray(target)\n",
    "  \n",
    "\n",
    "    a = np.zeros(len(processed_data))\n",
    "\n",
    "    for i in range(0,len(processed_data)):\n",
    "        a[i] = len(processed_data[i].split())\n",
    "\n",
    "    print(sum(a<=n) / len(processed_data))    \n",
    "    \n",
    "    processed_data = processed_data[a <= n]\n",
    "    target = target[a<=n]\n",
    "\n",
    "#####################################\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'Comment': processed_data})\n",
    "\n",
    "    X = df['Comment']\n",
    "    y = target\n",
    "\n",
    "\n",
    "    max_len2 = word_length(processed_data)\n",
    "\n",
    "    \n",
    "    return X,y,max_len2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.903206872632\n",
      "The maximal length of the sentences is 120 words\n"
     ]
    }
   ],
   "source": [
    "X2, y2, max_len4 = RNNprep2(train_data2,data2,index2,120,'binary')\n",
    "print('The maximal length of the sentences is',max_len4,'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34780</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120029</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65357</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105556</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23282</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75159</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71894</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135488</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159480</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9045</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43759</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124076</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53979</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28808</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51301</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28993</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35570</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13689</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90015</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142142</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77572</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50945</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84252</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10488</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23128</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8209</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44924</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153240</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23160</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137046</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120206</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128470</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110203</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136791</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107874</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141314</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19593</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102496</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18709</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71975</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140062</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25227</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143324</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131669</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143364</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37768</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83426</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41903</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51420</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63705</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94596</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39924</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99565</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113066</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118941</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105299</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108332 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "34780       0             0        0       0       0              0\n",
       "120029      0             0        0       0       0              0\n",
       "65357       0             0        0       0       0              0\n",
       "105556      0             0        0       0       0              0\n",
       "23282       0             0        0       0       0              0\n",
       "75159       0             0        0       0       0              0\n",
       "71894       1             0        1       0       1              0\n",
       "34570       0             0        0       0       0              0\n",
       "135488      0             0        0       0       0              0\n",
       "159480      0             0        0       0       0              0\n",
       "9045        1             0        0       0       1              0\n",
       "43759       0             0        0       0       0              0\n",
       "124076      0             0        0       0       0              0\n",
       "53979       0             0        0       0       0              0\n",
       "28808       0             0        0       0       0              0\n",
       "51301       0             0        0       0       0              0\n",
       "28993       0             0        0       0       0              0\n",
       "35570       0             0        0       0       0              0\n",
       "13689       0             0        0       0       0              0\n",
       "90015       0             0        0       0       0              0\n",
       "142142      0             0        0       0       0              0\n",
       "77572       0             0        0       0       0              0\n",
       "50945       0             0        0       0       0              0\n",
       "84252       0             0        0       0       0              0\n",
       "108107      0             0        0       0       0              0\n",
       "10488       0             0        0       0       0              0\n",
       "2504        0             0        0       0       0              0\n",
       "23128       0             0        0       0       0              0\n",
       "8209        0             0        0       0       0              0\n",
       "44924       0             0        0       0       0              0\n",
       "...       ...           ...      ...     ...     ...            ...\n",
       "153240      0             0        0       0       0              0\n",
       "23160       0             0        0       0       0              0\n",
       "137046      0             0        0       0       0              0\n",
       "120206      0             0        0       0       0              0\n",
       "128470      0             0        0       0       0              0\n",
       "110203      0             0        0       0       0              0\n",
       "136791      0             0        0       0       0              0\n",
       "107874      0             0        0       0       0              0\n",
       "141314      1             0        1       0       1              0\n",
       "19593       0             0        0       0       0              0\n",
       "102496      0             0        0       0       0              0\n",
       "18709       0             0        0       0       0              0\n",
       "71975       0             0        0       0       0              0\n",
       "140062      0             0        0       0       0              0\n",
       "25227       0             0        0       0       0              0\n",
       "143324      0             0        0       0       0              0\n",
       "131669      0             0        0       0       0              0\n",
       "143364      0             0        0       0       0              0\n",
       "37768       0             0        0       0       0              0\n",
       "83426       0             0        0       0       0              0\n",
       "41903       0             0        0       0       0              0\n",
       "51420       0             0        0       0       0              0\n",
       "39134       0             0        0       0       0              0\n",
       "63705       0             0        0       0       0              0\n",
       "94596       0             0        0       0       0              0\n",
       "39924       0             0        0       0       0              0\n",
       "99565       0             0        0       0       0              0\n",
       "113066      0             0        0       0       0              0\n",
       "118941      0             0        0       0       0              0\n",
       "105299      0             0        0       0       0              0\n",
       "\n",
       "[108332 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del y_train2['comment_text']\n",
    "del y_train2['binary']\n",
    "del y_train2['sum']\n",
    "y_train2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN2(input_shape, words_to_vec, word_to_index):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = trained_embedding_layer(words_to_vec, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "   \n",
    "\n",
    "    # When there is just one LSTM layer you don't need the batch of sequences since it is only forwarded to the \n",
    "    # dense unit \n",
    "\n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "\n",
    "    X = GRU(units = 64, activation = 'tanh')(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.6)(X)\n",
    "    # Propagate X through a Dense layer with sigmoid activation to get back a batch of 1-dim vectors.\n",
    "    \n",
    "    X = Dense(units = 6)(X)\n",
    "    # X = Dense(units = 64)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('sigmoid')(X)\n",
    "    # X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs = sentence_indices, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN2_improved(input_shape, words_to_vec, word_to_index):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = trained_embedding_layer(words_to_vec, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    X = Dropout(0.25)(embeddings)\n",
    "\n",
    "    X = Bidirectional(GRU(units = 64, activation = 'tanh',return_sequences = True))(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with sigmoid activation to get back a batch of 1-dim vectors.\n",
    "    \n",
    "    X = X = GlobalMaxPooling1D()(X)\n",
    "    X = Dense(units = 6)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('sigmoid')(X)\n",
    "    \n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs = sentence_indices, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 120, 100)          40000100  \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 64)                31680     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 40,032,170\n",
      "Trainable params: 32,070\n",
      "Non-trainable params: 40,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = RNN2((max_len4,), words_to_vec, word_to_index)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.005, decay=1e-8)\n",
    "model2.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_index2 = sentence_to_index(np.asarray(X_train2),word_to_index,max_len4,temp = 1)\n",
    "Y_train_index2 = np.asarray(y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 97498 samples, validate on 10834 samples\n",
      "Epoch 1/10\n",
      "97498/97498 [==============================] - 48s 488us/step - loss: 0.1324 - acc: 0.9675 - val_loss: 0.0603 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.97866, saving model to model2.h5\n",
      "Epoch 2/10\n",
      "97498/97498 [==============================] - 47s 478us/step - loss: 0.0622 - acc: 0.9785 - val_loss: 0.0582 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.97866 to 0.97868, saving model to model2.h5\n",
      "Epoch 3/10\n",
      "97498/97498 [==============================] - 47s 477us/step - loss: 0.0559 - acc: 0.9802 - val_loss: 0.0547 - val_acc: 0.9802\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.97868 to 0.98025, saving model to model2.h5\n",
      "Epoch 4/10\n",
      "97498/97498 [==============================] - 47s 478us/step - loss: 0.0526 - acc: 0.9812 - val_loss: 0.0545 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.98025\n",
      "Epoch 5/10\n",
      "97498/97498 [==============================] - 47s 478us/step - loss: 0.0511 - acc: 0.9815 - val_loss: 0.0543 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98025 to 0.98059, saving model to model2.h5\n",
      "Epoch 6/10\n",
      "97498/97498 [==============================] - 47s 477us/step - loss: 0.0485 - acc: 0.9822 - val_loss: 0.0523 - val_acc: 0.9809\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.98059 to 0.98092, saving model to model2.h5\n",
      "Epoch 7/10\n",
      "97498/97498 [==============================] - 47s 479us/step - loss: 0.0462 - acc: 0.9828 - val_loss: 0.0536 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.98092\n",
      "Epoch 8/10\n",
      "97498/97498 [==============================] - 47s 478us/step - loss: 0.0444 - acc: 0.9834 - val_loss: 0.0532 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.98092\n",
      "Epoch 9/10\n",
      "97498/97498 [==============================] - 47s 478us/step - loss: 0.0428 - acc: 0.9839 - val_loss: 0.0543 - val_acc: 0.9803\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.98092\n",
      "Epoch 10/10\n",
      "97498/97498 [==============================] - 47s 478us/step - loss: 0.0414 - acc: 0.9843 - val_loss: 0.0569 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.98092\n"
     ]
    }
   ],
   "source": [
    "checkpoint2 = ModelCheckpoint('model2.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "temp2 = model2.fit(X_train_index2, Y_train_index2,validation_split = 0.1,epochs = 10, batch_size = 512, callbacks=[checkpoint2])\n",
    "#validation_data = (X_val_index3,Y_val_index3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2511eae1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(temp2.history.keys())\n",
    "plt.plot(temp2.history['loss'])\n",
    "plt.plot(temp2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUokAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f24b7942160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(temp2.history.keys())\n",
    "plt.plot(temp2.history['acc'])\n",
    "plt.plot(temp2.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 120, 100)          40000100  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 120, 100)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 120, 128)          63360     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 120, 128)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 40,064,234\n",
      "Trainable params: 64,134\n",
      "Non-trainable params: 40,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2_improved = RNN2_improved((max_len4,), words_to_vec, word_to_index)\n",
    "model2_improved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.005, decay=1e-8)\n",
    "model2_improved.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 97498 samples, validate on 10834 samples\n",
      "Epoch 1/10\n",
      "97498/97498 [==============================] - 113s 1ms/step - loss: 0.0877 - acc: 0.9707 - val_loss: 0.0912 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.97931, saving model to model2_improved.h5\n",
      "Epoch 2/10\n",
      "97498/97498 [==============================] - 111s 1ms/step - loss: 0.0578 - acc: 0.9793 - val_loss: 0.0763 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.97931 to 0.98049, saving model to model2_improved.h5\n",
      "Epoch 3/10\n",
      "97498/97498 [==============================] - 111s 1ms/step - loss: 0.0540 - acc: 0.9801 - val_loss: 0.0707 - val_acc: 0.9803\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.98049\n",
      "Epoch 4/10\n",
      "97498/97498 [==============================] - 111s 1ms/step - loss: 0.0515 - acc: 0.9810 - val_loss: 0.0712 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.98049 to 0.98115, saving model to model2_improved.h5\n",
      "Epoch 5/10\n",
      "97498/97498 [==============================] - 111s 1ms/step - loss: 0.0493 - acc: 0.9815 - val_loss: 0.0756 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98115 to 0.98129, saving model to model2_improved.h5\n",
      "Epoch 6/10\n",
      "97498/97498 [==============================] - 111s 1ms/step - loss: 0.0483 - acc: 0.9819 - val_loss: 0.0676 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.98129\n",
      "Epoch 7/10\n",
      "97498/97498 [==============================] - 111s 1ms/step - loss: 0.0474 - acc: 0.9822 - val_loss: 0.0739 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.98129\n",
      "Epoch 8/10\n",
      "97498/97498 [==============================] - 111s 1ms/step - loss: 0.0461 - acc: 0.9826 - val_loss: 0.0709 - val_acc: 0.9809\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.98129\n",
      "Epoch 9/10\n",
      "97498/97498 [==============================] - 111s 1ms/step - loss: 0.0456 - acc: 0.9828 - val_loss: 0.0713 - val_acc: 0.9812\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.98129\n",
      "Epoch 10/10\n",
      "97498/97498 [==============================] - 111s 1ms/step - loss: 0.0458 - acc: 0.9825 - val_loss: 0.0652 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.98129 to 0.98146, saving model to model2_improved.h5\n"
     ]
    }
   ],
   "source": [
    "checkpoint2_improved = ModelCheckpoint('model2_improved.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "temp2_improved = model2_improved.fit(X_train_index2, Y_train_index2,validation_split = 0.1,epochs = 10, batch_size = 512, callbacks=[checkpoint2_improved])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA6UrX1o74V+HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f670045ff60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(temp2_improved.history.keys())\n",
    "plt.plot(temp2_improved.history['loss'])\n",
    "plt.plot(temp2_improved.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f67004d6dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(temp2_improved.history.keys())\n",
    "plt.plot(temp2_improved.history['acc'])\n",
    "plt.plot(temp2_improved.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2_imp = load_model(\"model2_improved.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y_test2['comment_text']\n",
    "del y_test2['sum']\n",
    "del y_test2['binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27084/27084 [==============================] - 16s 587us/step\n",
      "\n",
      "Test accuracy =  98.1471212131 %\n"
     ]
    }
   ],
   "source": [
    "X_test_index2 = sentence_to_index(np.asarray(X_test2), word_to_index, max_len = max_len4, temp = 1)\n",
    "Y_test_index2 = np.asarray(y_test2)\n",
    "loss2, acc2 = model2_imp.evaluate(X_test_index2, Y_test_index2)\n",
    "print()\n",
    "print(\"Test accuracy = \",acc2 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict2(phrases):\n",
    "    \n",
    "    m = phrases.shape[0]\n",
    "    \n",
    "    y_pred2 = model2_imp.predict(sentence_to_index(phrases, word_to_index, max_len4, temp = 1))\n",
    "    \n",
    "    predict = np.zeros((m,6))\n",
    "    \n",
    "    for i in range(0,m):\n",
    "        for j in range(0,6):\n",
    "\n",
    "            if y_pred2[i,j] > 0.5:\n",
    "                predict[i,j] = 1\n",
    "            else:\n",
    "                predict[i,j] = 0\n",
    "    predict\n",
    "        \n",
    "    return predict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples = np.asarray((\"You are an asshole\",\"You are dead\",\n",
    "                       \"I do not really care about this\",\"I like this movie\",\"I will kill them all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93313777,  0.3603996 ,  0.87037194,  0.04358162,  0.8532306 ,\n",
       "         0.11601751],\n",
       "       [ 0.76458448,  0.18274066,  0.27549881,  0.17343172,  0.52819169,\n",
       "         0.08317558],\n",
       "       [ 0.0784456 ,  0.01596765,  0.03552206,  0.01396521,  0.02803898,\n",
       "         0.00919752],\n",
       "       [ 0.06210713,  0.01732511,  0.0315634 ,  0.01205957,  0.01927907,\n",
       "         0.00815657],\n",
       "       [ 0.63853103,  0.12435254,  0.17950298,  0.59333283,  0.21743433,\n",
       "         0.05745389]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2_imp.predict(sentence_to_index(examples, word_to_index, max_len4, temp = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.,  0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2 = predict2(examples)\n",
    "predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_random2 = np.zeros((len(y_test2),6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(y_test2)):\n",
    "    for j in range(0,6):\n",
    "        \n",
    "        y_random2[i,j] = np.random.binomial(n=1, p = share2[j])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random accuracy is 71.1859400384 %\n"
     ]
    }
   ],
   "source": [
    "# calculate the accuracy of the random benchmark model\n",
    "temp = np.zeros(len(y_test2))\n",
    "error2 = np.zeros(len(y_test2))\n",
    "\n",
    "for i in range(0,len(y_test2)):\n",
    "    temp = np.sum(abs(y_random2 - np.asarray(y_test2)),axis = 1)\n",
    "    if temp[i] > 0:\n",
    "        error2[i] = 1\n",
    "    else:\n",
    "        error2[i] = 0\n",
    "        \n",
    "acc_random2 = 1 - sum(error2)/len(y_test2)\n",
    "\n",
    "print(\"The random accuracy is\",acc_random2*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a2 = np.asarray(X_test2)\n",
    "y_predict2 = predict2(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1_score of the model is: 0.576397909942\n",
      "The F1_score of the benchmark is: 0.0371604964048\n"
     ]
    }
   ],
   "source": [
    "F1_model2  = sklearn.metrics.f1_score(np.asarray(y_test2),y_predict2,average='macro')\n",
    "F1_random2 = sklearn.metrics.f1_score(np.asarray(y_test2),y_random2,average='macro')\n",
    "\n",
    "print(\"The F1_score of the model is:\",F1_model2)\n",
    "print(\"The F1_score of the benchmark is:\",F1_random2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
